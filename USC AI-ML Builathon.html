<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>USC AI-ML Builathon</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h2 id="prompt-scoring-prompt-format">Prompt-scoring prompt format</h2>
<pre><code>Challenge goal: &lt;challenge goal&gt;

Player's prompt: &lt;player's prompt&gt;

I gave the challenge goal above to a player and asked them to propose a prompt for an AI model. Their prompt is above. I want you to score their prompt on a score of 0-100 while keeping the following in mind. Using a prompt that is the same as, or very similar to the challenge goal text may lead to a "correct" response, but it may not be the best prompt. A better prompt might need to include more context such as, but not limited to:
* Format/Data Structure (e.g., number of bullets, JSON, table)
* Tone/Persona (e.g., writing as a pirate, explaining to a child)
* Ambiguity Reduction (e.g., defining vague terms like "best" or "current")
* Procedural Guidance (e.g., requesting step-by-step reasoning or Chain-of-Thought)
* Exclusionary Constraints (e.g., telling the AI what to omit, like greetings or jargon)
* Tool Usage (e.g., explicitly requesting the use of the Google Search tool)

You should provide your response in the following format:

Score: &lt;score&gt;/100
Feedback: &lt;explanation and reasoning to the assigned score, and how the user's prompt could be improved&gt;
</code></pre>
<h2 id="prompt-quest-challenge-goals">Prompt Quest Challenge Goals</h2>
<h3 id="challenge-goal-give-me-five-facts-about-the-roman-empire.">1. Challenge Goal: “Give me five facts about the Roman Empire.”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> A basic request for facts provides the AI with zero instruction on presentation. The LLM will provide five facts, but the format and length are arbitrary—it might use five dense paragraphs, making the information hard to scan quickly or integrate into a study guide.</li>
<li><strong>Why it Fails:</strong> The model does not know <em>how</em> the user wants the information delivered, wasting the opportunity to tailor the output.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Constraint Setting (Format and Length):</strong> The player needs to specify the exact output format (e.g., “Respond with a <strong>Markdown table</strong> with two columns: ‘Fact Number’ and ‘Detail’”) and the length (e.g., “each detail must be a single, concise sentence”).</li>
</ul>
<h3 id="challenge-goal-explain-the-concept-of-supply-and-demand.">2. Challenge Goal: “Explain the concept of supply and demand.”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> The response will be dry, textbook-like, and probably too formal for a high school student or someone new to the topic. It lacks an engaging hook or contextual relevance.</li>
<li><strong>Why it Fails:</strong> Lack of <strong>persona</strong> and <strong>audience</strong> control leads to generic, unengaging content that may not be useful for the specific learner.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Persona and Audience:</strong> The player must instruct the AI to adopt a role (e.g., “Act as a friendly, patient economics tutor”) and tailor the explanation (e.g., “Explain it using the <strong>analogy of a fast-food restaurant</strong> to someone who is 15 years old”).</li>
</ul>
<h3 id="challenge-goal-what-are-three-ways-to-save-energy-at-home">3. Challenge Goal: “What are three ways to save energy at home?”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> The LLM will often pad the response with conversational boilerplate. It might preface the list with an introductory sentence (“That’s a great question! Here are three ways…”) and may include a concluding remark or a generic safety disclaimer, cluttering the response.</li>
<li><strong>Why it Fails:</strong> The response is padded with unnecessary conversational text, drastically reducing its <strong>efficiency</strong> and clarity.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Exclusionary Constraints:</strong> The player must explicitly tell the AI what <strong>not</strong> to do (e.g., “Do not include any greeting, introduction, or closing remarks. Start directly with the first item in the list.”)</li>
</ul>
<h3 id="challenge-goal-what-is-the-best-way-to-travel-from-los-angeles-to-new-york">4. Challenge Goal: “What is the best way to travel from Los Angeles to New York?”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> The term “best” is ambiguous. Best for cost? Speed? Environmental impact? The model will likely choose one default (e.g., flight) or list all options without the necessary criteria to make a judgment.</li>
<li><strong>Why it Fails:</strong> The goal uses an unqualified, vague <strong>superlative</strong> (“best”) which the LLM must attempt to define on its own, potentially guessing the user’s true intent incorrectly.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Ambiguity Reduction:</strong> The player must define the vague term. The prompt should specify the constraint (e.g., “Find the way that is the <strong>most cost-effective</strong> for two people and takes <strong>no more than 5 days</strong> of travel time.”)</li>
</ul>
<h3 id="challenge-goal-list-the-ingredients-for-a-basic-chocolate-chip-cookie-recipe.">5. Challenge Goal: “List the ingredients for a basic chocolate chip cookie recipe.”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> The output will be a simple paragraph or a basic bulleted list. While readable, it’s very difficult to use if a program needs to process the data (e.g., a recipe app, a shopping list generator, or another LLM function).</li>
<li><strong>Why it Fails:</strong> The output is unstructured text, which is fine for reading but poor for any kind of programmatic <strong>data processing or integration</strong>.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Structured Data Output:</strong> The player needs to demand a specific, machine-readable format, such as <strong>JSON</strong> or a <strong>CSV-style output</strong>, with defined fields (e.g., “Output the result as a JSON array where each object has fields for <code>ingredientName</code>, <code>quantity</code>, and <code>unit</code>.”).</li>
</ul>
<h3 id="challenge-goal-if-a-car-travels-60-miles-per-hour-and-leaves-at-1000-am-what-time-will-it-arrive-at-a-city-210-miles-away">6. Challenge Goal: “If a car travels 60 miles per hour and leaves at 10:00 AM, what time will it arrive at a city 210 miles away?”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> The LLM will often provide the correct answer immediately. However, if the problem were more complex (e.g., involving rest stops or changing speeds), a simple answer hides the logic. If the answer is wrong, there’s no way for the user to debug or verify the AI’s calculation process.</li>
<li><strong>Why it Fails:</strong> Lack of <strong>procedural guidance</strong> means the output is a “black box” answer without verifiable reasoning, undermining trust in the solution.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Procedural Guidance (Chain-of-Thought):</strong> The player must instruct the AI to break down the calculation and show its work (e.g., “First, calculate the time in hours using the formula. Second, convert the remainder to minutes. Third, add the total travel time to the start time. Show all three steps before providing the final answer.”).</li>
</ul>
<h3 id="challenge-goal-what-is-the-stock-price-of-tesla-tsla-right-now">7. Challenge Goal: “What is the stock price of Tesla (TSLA) right now?”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> The LLM’s internal knowledge cutoff means its training data is likely several months old, making any answer for a current stock price factually wrong.</li>
<li><strong>Why it Fails:</strong> The goal requires <strong>real-time, grounded knowledge</strong>, which the base LLM lacks without explicit <strong>tool invocation</strong>.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Tool Usage (Grounding):</strong> The player must explicitly ask the AI to <strong>use the Google Search tool</strong> to access real-time data and then request a specific, verifiable data structure for the answer (e.g., “Use the search tool to find the most recent closing price for TSLA and state the result in the format: ‘TSLA’s last closing price was $[PRICE] on [DATE].’”).</li>
</ul>
<h3 id="challenge-goal-write-a-short-history-of-coffee.">8. Challenge Goal: “Write a short history of coffee.”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> This is vague on scope, tone, and length. The output will be a generic overview, likely covering everything from the discovery in Ethiopia to modern coffee culture, which may not align with the user’s specific need (e.g., writing a paper on 17th-century trade).</li>
<li><strong>Why it Fails:</strong> The lack of <strong>multiple, simultaneous constraints</strong> allows the AI to choose the easiest, most generic answer, rather than a highly targeted one.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Multi-Constraint Prompting:</strong> The player must combine several skills: <strong>Persona</strong> (“Act as a sarcastic barista”), <strong>Length</strong> (“Respond in exactly <strong>three paragraphs</strong>”), and <strong>Content</strong> (“The history must <strong>focus only</strong> on the introduction of coffee to Europe and the rise of coffee houses.”).</li>
</ul>
<h3 id="challenge-goal-draft-a-quick-email-to-my-boss-asking-for-a-week-off.">9. Challenge Goal: “Draft a quick email to my boss asking for a week off.”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> The model assumes the necessary dates, reasons, and subject line, leading to a generic response that the user still has to edit heavily.</li>
<li><strong>Why it Fails:</strong> It doesn’t define the <em>template</em> needed for a real-world task or define the required <strong>input variables</strong> that need to be filled in by the user.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Input Guidance &amp; System Role:</strong> The player must instruct the AI on its <strong>System Role</strong> (e.g., “You are a professional HR consultant drafting a template”) and define the missing <strong>Input Variables</strong> that need to be filled (e.g., “The email must include bolded placeholders for <strong>[START_DATE]</strong>, <strong>[END_DATE]</strong>, and <strong>[REASON]</strong>. Do not use any filler text outside the email body.”).</li>
</ul>
<h3 id="challenge-goal-what-are-the-benefits-of-solar-power-versus-wind-power">10. Challenge Goal: “What are the benefits of solar power versus wind power?”</h3>
<ul>
<li><strong>The Flaw in the Goal:</strong> The LLM will give a balanced, static list of pros and cons, which is difficult to compare effectively for making a real-world decision (e.g., deciding which one to invest in).</li>
<li><strong>Why it Fails:</strong> The response format is not designed for <strong>comparison</strong> or <strong>decision support</strong>, which is the likely intent of the query.</li>
<li><strong>Player’s Improvement Focus:</strong> <strong>Comparative &amp; Formatting Task:</strong> The player must ask the AI to perform a structured analysis and present the results in a comparative format. (e.g., “Compare the two technologies using a <strong>Markdown table</strong> with four columns: ‘Factor’, ‘Solar Power’, ‘Wind Power’, and ‘Conclusion/Score (1-10).’ The comparison must be based specifically on <strong>residential installation</strong> viability in a cloudy climate.”)</li>
</ul>
</div>
</body>

</html>
